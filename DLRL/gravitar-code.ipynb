{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RL v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Initialise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZefME0MTvA"
      },
      "source": [
        "# The final submission is a Deep Learning Advantage Actor/Critic Network (A2C). I also attempted to write a genetic RL algorithm as in https://arxiv.org/pdf/1712.06567.pdf which didn't\n",
        "# work out as well as in the paper given the platform constraints but I have left the code in for posterity (see the notes at the bottom of this file for info on that)\n",
        "# The reason for choosing this is that it combines value-based learning (calculating expected return given action from current state) and policy-based\n",
        "# (building up knowledge of a policy). Rewards in Gravitar are sparse. Plain policy gradient approaches aren't great for Gravitar because of the reward sparsity.\n",
        "# The agent can take bad (inefficient or stupid) actions and still end up with a decent score at the end of the run so differentiating between action types isn't that easy.\n",
        "# I originally attempted a Duelling DQN as a value-based learner but found it incredibly slow and also the high variability caused issues. It took a very long time\n",
        "# to train and didn't seem to work well with the preprocessing code when that was implemented to try speed up learning.\n",
        "# The critic calculates the q value of taken an action from the state and the actor will modify its policy based on that. The use of an advantage function accounts for the\n",
        "# state mean which reduces the variability. My particular implementation uses the image based Gravitar and for this reason uses a convolutional net rather than the \n",
        "# linear layers. The CNN has been kept quite small to ensure training is reasonable but this may be at the expense of long-term performance. My implementation has a couple\n",
        "# of strange quirks. Firstly, using the Softmax function as in the referenced code caused a lot of numerical stability issues with the categorical distribution. Tweaking this\n",
        "# to Softmax(x - max(x)) to try prevent underflow or overflow didn't work. I also added gradient clipping and lowered the learning rate and the issues persisted which suggested this wasn't\n",
        "# gradient explosion. Eventually the solution to stabilise turned out to be to use Sigmoid which has roughly the same shape but constrains outputs between 0 and 1 so that the categorical distribution\n",
        "# input is never < 0. I used MSE loss instead of smooth l1 loss. This was mostly while experimenting with gradient issues but I found that it produced more consistent results too which makes sense.\n",
        "# The aim of preprocessing is as in: https://www.nature.com/articles/nature14236. Prevent sprite flickering due to Atari limitations by taking max of frames,\n",
        "# downsample the Atari screen to a smaller size to make processing faster (and make it grayscale since the net doesn't benefit from colour) and also stack frames to learn better.\n",
        "# Since a lot of movement occurs in Gravitar, the agent learning based on this is useful. Also OpenAI Gym annoyingly gives numpy arrays in the wrong order for CHW which Pytorch expects so permute.\n",
        "# The A2C core code is heavily based on https://github.com/seungeunrho/minimalRL/blob/master/actor_critic.py, MIT License\n",
        "# Preprocessing code is based on https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/blob/master/DQN/utils.py, MIT License\n",
        "# ESDQN code is based on https://github.com/atgambardella/pytorch-es/blob/master/model.py, MIT License\n",
        "\n",
        "# imports\n",
        "import gym\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import math\n",
        "import copy\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.00005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 10000\n",
        "batch_size    = 32\n",
        "video_every   = 25\n",
        "print_every   = 5\n",
        "n_rollout = 10\n",
        "\n",
        "\"\"\"\n",
        "Start preprocessing code. This is very generic stuff so it's basically just a stripped back version of the repository mentioned in the header\n",
        "\"\"\"\n",
        "class MaxFrameWrapper(gym.Wrapper):\n",
        "  def __init__(self, env, repeat):\n",
        "      super(MaxFrameWrapper, self).__init__(env)\n",
        "      self.repeat = repeat\n",
        "      self.shape = env.observation_space.low.shape\n",
        "      self.frame_buffer = np.zeros((2, ) + self.shape, dtype = np.uint8)\n",
        "\n",
        "  def step(self, action):\n",
        "      t_reward = 0.0\n",
        "      done = False\n",
        "      for i in range(self.repeat):\n",
        "          obs, reward, done, info = self.env.step(action)\n",
        "          t_reward += reward\n",
        "          self.frame_buffer[i % 2] = obs\n",
        "          if done:\n",
        "              break\n",
        "\n",
        "      max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
        "      return max_frame, t_reward, done, info\n",
        "\n",
        "  def reset(self):\n",
        "      obs = self.env.reset()\n",
        "      self.frame_buffer = np.zeros((2, ) + self.shape, dtype = np.uint8)\n",
        "      self.frame_buffer[0] = obs\n",
        "      return obs\n",
        "\n",
        "class PreprocessFrameWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "      super(PreprocessFrameWrapper, self).__init__(env)\n",
        "      self.shape = (shape[2], shape[0], shape[1])\n",
        "      self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
        "\n",
        "  def observation(self, obs):\n",
        "      new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "      resized_screen = cv2.resize(new_frame, self.shape[1:], interpolation=cv2.INTER_AREA)\n",
        "      new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
        "      new_obs = new_obs / 255.0\n",
        "      return new_obs\n",
        "\n",
        "class StackFramesWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self, env, repeat):\n",
        "      super(StackFramesWrapper, self).__init__(env)\n",
        "      self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(repeat, axis=0), env.observation_space.high.repeat(repeat, axis=0), dtype=np.float32)\n",
        "      self.stack = collections.deque(maxlen = repeat)\n",
        "\n",
        "  def reset(self):\n",
        "      self.stack.clear()\n",
        "      observation = self.env.reset()\n",
        "      for _ in range(self.stack.maxlen):\n",
        "          self.stack.append(observation)\n",
        "\n",
        "      return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "  def observation(self, observation):\n",
        "      self.stack.append(observation)\n",
        "\n",
        "      return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = MaxFrameWrapper(env, 4)\n",
        "  env = PreprocessFrameWrapper(env, (84,84,1))\n",
        "  env = StackFramesWrapper(env, 4)\n",
        "  return env\n",
        "\"\"\"\n",
        "End preprocessing code\n",
        "\"\"\"\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, env, learn_rate, gamma):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    self.learn_rate = learn_rate\n",
        "    self.gamma = gamma\n",
        "    self.input_size = env.observation_space.shape\n",
        "    self.output_size = env.action_space.n\n",
        "    self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "    self.feature_size = self.conv_block(torch.autograd.Variable(torch.zeros(1, *self.input_size))).view(1, -1).shape[1]\n",
        "    self.critic_linear = nn.Linear(self.feature_size, 1)\n",
        "    self.actor_linear = nn.Linear(self.feature_size, self.output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.data = []\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(), lr=learn_rate)\n",
        "\n",
        "  def load(self, checkpoint):\n",
        "    params = torch.load('drive/My Drive/rl_training/ac_' + checkpoint + '.chkpt')\n",
        "    self.load_state_dict(params['model'])\n",
        "    self.optimizer.load_state_dict(params['optimizer'])\n",
        "\n",
        "  def save(self, checkpoint):\n",
        "    torch.save({'model': self.state_dict(), 'optimizer': self.optimizer.state_dict()}, 'drive/My Drive/rl_training/ac_' + checkpoint + '.chkpt')\n",
        "\n",
        "  def actor(self, x):\n",
        "      x = self.conv_block(x)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "      x = self.actor_linear(x)\n",
        "      prob = self.sigmoid(x)\n",
        "      return prob\n",
        "  \n",
        "  def critic(self, x):\n",
        "      x = self.conv_block(x)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "      r = self.critic_linear(x)\n",
        "      return r\n",
        "  \n",
        "  def put_data(self, transition):\n",
        "      self.data.append(transition)\n",
        "      \n",
        "  \"\"\"\n",
        "  make_batch and train_net are basically the same as in the original cited in the header, except for my changes to the loss function and gradient clipping\n",
        "  \"\"\"\n",
        "  def make_batch(self):\n",
        "      s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "      for transition in self.data:\n",
        "          s,a,r,s_prime,done = transition\n",
        "          s_lst.append(s)\n",
        "          a_lst.append([a])\n",
        "          r_lst.append([r/100.0])\n",
        "          s_prime_lst.append(s_prime)\n",
        "          done_mask = 0.0 if done else 1.0\n",
        "          done_lst.append([done_mask])\n",
        "      \n",
        "      s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                                              torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                                              torch.tensor(done_lst, dtype=torch.float)\n",
        "      self.data = []\n",
        "      return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
        "\n",
        "  def train_net(self):\n",
        "      s, a, r, s_prime, done = self.make_batch()\n",
        "      td_target = r + self.gamma * self.critic(s_prime) * done\n",
        "      delta = td_target - self.critic(s)\n",
        "      \n",
        "      pi = self.actor(s)\n",
        "      pi_a = pi.gather(1,a)\n",
        "      loss = -torch.log(pi_a) * delta.detach() + F.mse_loss(self.critic(s), td_target.detach())\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()\n",
        "      torch.nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
        "\n",
        "\n",
        "class GenomeQNet(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(GenomeQNet, self).__init__()\n",
        "    self.input_size = (input_size[2], input_size[0], input_size[1])\n",
        "    self.output_size = output_size\n",
        "    self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 16, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    self.feature_size = self.conv_block(torch.autograd.Variable(torch.zeros(1, *self.input_size))).view(1, -1).shape[1]\n",
        "    self.linear_layer = nn.Sequential(\n",
        "        nn.Linear(self.feature_size, 256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.out = nn.Linear(256, self.output_size)\n",
        "    self.scores = []\n",
        "\n",
        "  def forward(self, state):\n",
        "    z = self.get_from_frame(state)\n",
        "    features = self.conv_block(z)\n",
        "    features = features.view(features.shape[0], -1)\n",
        "    linear = self.linear_layer(features)\n",
        "    out = self.out(linear)\n",
        "    return out\n",
        "\n",
        "  def get_genome(self):\n",
        "    genome = torch.Tensor()\n",
        "    for p in self.parameters():\n",
        "      genome = torch.cat((genome, p.flatten()), 0)\n",
        "    return genome\n",
        "\n",
        "  def get_score(self):\n",
        "    return torch.Tensor(self.scores).mean().item()\n",
        "\n",
        "  def update_params(self, params):\n",
        "    for i, p in enumerate(self.parameters()):\n",
        "      p.data.copy_(params[i])\n",
        "\n",
        "  def add_score(self, score):\n",
        "    self.scores.append(score)\n",
        "\n",
        "  def clear_scores(self):\n",
        "    self.scores = []\n",
        "\n",
        "  def sample_action(self, obs, epsilon):\n",
        "    coin = random.random()\n",
        "    if epsilon != -1 and coin < epsilon:\n",
        "        return random.randint(0,1)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        return self.forward(obs).argmax().item()\n",
        "\n",
        "  def get_from_frame(self, frame):\n",
        "    frame = torch.from_numpy(np.ascontiguousarray(frame, dtype=np.float32))\n",
        "    return frame.unsqueeze(0).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "\"\"\"\n",
        "This QNet is based on an implementation that was used in Evolutionary Strategies (ES)\n",
        "here: https://github.com/atgambardella/pytorch-es/blob/master/model.py so I thought it might be more successful\n",
        "with my GA due to the introduction of the LSTM cells for memory. It was better than the GenomeQNet but not enough\n",
        "\"\"\"\n",
        "class ESQNet(GenomeQNet):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(ESQNet, self).__init__(input_size, output_size)\n",
        "    self.input_size = (input_size[2], input_size[0], input_size[1])\n",
        "    self.output_size = output_size\n",
        "    self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.SELU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.SELU(),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.SELU()\n",
        "        )\n",
        "    self.feature_size = self.conv_block(torch.autograd.Variable(torch.zeros(1, *self.input_size))).view(1, -1).shape[1]\n",
        "    self.lstm = nn.LSTMCell(self.feature_size, 256)\n",
        "    self.out = nn.Linear(256, self.output_size)\n",
        "    self.scores = []\n",
        "    self.cx = torch.autograd.Variable(torch.zeros(1, 256))\n",
        "    self.hx = torch.autograd.Variable(torch.zeros(1, 256))\n",
        "\n",
        "  def forward(self, state):\n",
        "    z = self.get_from_frame(state)\n",
        "    features = self.conv_block(z)\n",
        "    features = features.view(-1, self.feature_size)\n",
        "    self.hx, self.cx = self.lstm(features, (self.hx, self.cx))\n",
        "    features = self.hx\n",
        "    out = F.softmax(self.out(features), dim = -1)\n",
        "    return out\n",
        "\n",
        "\"\"\"\n",
        "Here's the container for my not so good GA algorithm.\n",
        "I originally tried it with GenomeQNet but found I got better results by using\n",
        "ESQNet which is similar except with the introduction of LSTMCells between the convolutional\n",
        "and the action layer.\n",
        "\"\"\"\n",
        "class GeneticQNetContainer():\n",
        "  def __init__(self, input_size, output_size, hyperparameters = {}):\n",
        "    self._input_size = input_size\n",
        "    self._output_size = output_size\n",
        "    self._initial_qnet = ESQNet(input_size, output_size)\n",
        "    self._genome_param_sizes = self.init_genome_param_sizes(self._initial_qnet)\n",
        "    self._population = []\n",
        "    self._best_individual = None\n",
        "    self._evaluations_per_individual = 1\n",
        "    self._mutation_chance = 0.1\n",
        "    self._mutation_factor = 0.02\n",
        "    self._tournament_size = 20\n",
        "    self._cull_after = 0.5\n",
        "    self._population_size = 100\n",
        "    self._elapsed_generations = 0\n",
        "    self._use_crossover = True\n",
        "    self._hyperparameter_keys = {\n",
        "        'evaluations_per_individual': None,\n",
        "        'mutation_chance': None,\n",
        "        'mutation_factor': None,\n",
        "        'tournament_size': None,\n",
        "        'cull_after': None,\n",
        "        'population_size': None,\n",
        "        'use_crossover': None\n",
        "    }\n",
        "    self.set_hyperparameters(hyperparameters)\n",
        "    self.reset_remaining_tasks()\n",
        "  \n",
        "  def set_hyperparameters(self, hyperparameters):\n",
        "    for p in hyperparameters:\n",
        "      if p in self._hyperparameter_keys:\n",
        "        setattr(self, '_' + p, hyperparameters[p])\n",
        "\n",
        "  def no_tasks_left(self):\n",
        "    return len(self._tasks_remaining_in_generation) == 0\n",
        "\n",
        "  def reset_remaining_tasks(self):\n",
        "    self._tasks_remaining_in_generation = []\n",
        "    for _ in range(self._evaluations_per_individual):\n",
        "      self._tasks_remaining_in_generation += list(range(0, self._population_size))\n",
        "    random.shuffle(self._tasks_remaining_in_generation)\n",
        "\n",
        "  def init_genome_param_sizes(self, qnet):\n",
        "    s = []\n",
        "    params = qnet.parameters()\n",
        "    for p in params:\n",
        "      s.append(p.shape)\n",
        "    return s\n",
        "\n",
        "  def get_genome_size(self):\n",
        "    size = 0\n",
        "    for s in self._genome_param_sizes:\n",
        "      size += s.numel()\n",
        "    return size\n",
        "\n",
        "  \"\"\"\n",
        "  Breed 2 nets using ordered crossover\n",
        "  \"\"\"\n",
        "  def breed(self, net0_params, net1_params):\n",
        "    child = torch.zeros(net0_params.shape.numel())\n",
        "    a = random.randrange(0, net0_params.shape.numel() + 1)\n",
        "    b = random.randrange(0, net1_params.shape.numel() + 1)\n",
        "    min_ab = min(a, b)\n",
        "    max_ab = max(a, b)\n",
        "    child[min_ab:max_ab] = net0_params[min_ab:max_ab]\n",
        "    gapped = net1_params.clone()\n",
        "    gapped[min_ab:max_ab] = torch.zeros(net1_params.shape.numel())[min_ab:max_ab]\n",
        "    child = child + gapped\n",
        "    return child\n",
        "\n",
        "  def tournament_pick(self, choosable_genomes):\n",
        "    if self._tournament_size > len(choosable_genomes):\n",
        "      self._tournament_size = len(choosable_genomes)\n",
        "    best = None\n",
        "    for i in range(self._tournament_size):\n",
        "        i = random.randrange(0, len(choosable_genomes))\n",
        "        if best == None or self._population[i].get_score() > self._population[best].get_score():\n",
        "            best = i\n",
        "    return choosable_genomes[best]\n",
        "\n",
        "  def rank_population(self, population):\n",
        "    sorted_pop = sorted(population, key=lambda x: x.get_score(), reverse=True)\n",
        "    return sorted_pop\n",
        "\n",
        "  def cull_individuals_with_worst_genomes(self, ordered_population):\n",
        "    cull_point = int(math.ceil(len(ordered_population) * self._cull_after))\n",
        "    return ordered_population[:cull_point]\n",
        "\n",
        "  def get_elite(self, ordered_population):\n",
        "    return ordered_population[0]\n",
        "\n",
        "  def breed_population_genomes(self, choosable_genomes, number):\n",
        "    children = []\n",
        "    for i in range(number):\n",
        "        parent0 = self.tournament_pick(choosable_genomes)\n",
        "        parent1 = self.tournament_pick(choosable_genomes)\n",
        "        children.append(self.breed(parent0, parent1))\n",
        "    return children\n",
        "\n",
        "  \"\"\"\n",
        "  mutation_chance is a hyperparameter. Not in the paper but I think it might be useful to have the option\n",
        "  mutation_factor is a hyperparameter. Default 0.02 as here: https://arxiv.org/pdf/1712.06567.pdf\n",
        "  \"\"\"\n",
        "  def mutate_noise(self, genome):\n",
        "    size = genome.shape.numel()\n",
        "    noise = torch.randn(size) * self._mutation_factor\n",
        "    mutation_attempts = torch.rand(size)\n",
        "    threshold_met = mutation_attempts < self._mutation_chance\n",
        "    mutated_whole = genome + noise\n",
        "    new_genome = torch.where(threshold_met, mutated_whole, genome)\n",
        "    return new_genome\n",
        "\n",
        "  def mutate_population_genomes(self, genomes):\n",
        "    mutated = []\n",
        "    for g in genomes:\n",
        "      g_m = self.mutate_noise(g)\n",
        "      mutated.append(g_m)\n",
        "    return mutated\n",
        "\n",
        "  def build_params_from_genome(self, genome):\n",
        "    start = 0\n",
        "    params = []\n",
        "    for size in self._genome_param_sizes:\n",
        "      flat_size = size.numel()\n",
        "      genome_block = genome[start:start + flat_size].clone()\n",
        "      reshaped = genome_block.reshape(size)\n",
        "      params.append(reshaped)\n",
        "      start += flat_size\n",
        "    return params\n",
        "\n",
        "  def init_random_population(self):\n",
        "    self._population = []\n",
        "    for i in range(self._population_size):\n",
        "      qnet = ESQNet(self._input_size, self._output_size)\n",
        "      self._population.append(qnet)\n",
        "\n",
        "  def next_generation(self):\n",
        "    genomes = []\n",
        "    population_score = []\n",
        "    for ind in self._population:\n",
        "      population_score.append(ind.get_score())\n",
        "    generation_mean_score = torch.Tensor(population_score).mean().item()\n",
        "    print('Generation ' + str(self._elapsed_generations) + ' completed. Mean score: ' + str(generation_mean_score))\n",
        "    ranked_population = self.rank_population(self._population)\n",
        "    elite = self.get_elite(ranked_population)\n",
        "    if self._use_crossover:\n",
        "      culled_population = self.cull_individuals_with_worst_genomes(ranked_population)\n",
        "      population_size = len(self._population)\n",
        "      for individual in culled_population:\n",
        "        genomes.append(individual.get_genome())\n",
        "      bred_genomes = self.breed_population_genomes(genomes, population_size - 1)\n",
        "      genomes = bred_genomes\n",
        "    else:\n",
        "      for individual in ranked_population[:5]:\n",
        "        genomes.append(individual.get_genome())\n",
        "      genomes = genomes * 10\n",
        "      genomes = genomes[0:len(genomes) - 1]\n",
        "    mutated_genomes = self.mutate_population_genomes(genomes)\n",
        "    if self._best_individual is None or self._best_individual.get_score() < elite.get_score():\n",
        "      self._best_individual = copy.copy(elite)\n",
        "    self._population[0] = elite\n",
        "    params = None\n",
        "    for i, g in enumerate(mutated_genomes):\n",
        "      params = self.build_params_from_genome(g)\n",
        "      self._population[i + 1].update_params(params)\n",
        "    params = None\n",
        "    for ind in self._population:\n",
        "      ind.clear_scores()\n",
        "    self._elapsed_generations += 1\n",
        "\n",
        "  def get_population_dict(self):\n",
        "    param_dict = {}\n",
        "    for i, individual in self._population:\n",
        "      param_dict['individual_' + str(i)] = individual.state_dict()\n",
        "    return param_dict\n",
        "\n",
        "  def load(self, generation):\n",
        "    self.init_random_population()\n",
        "    params = torch.load('drive/My Drive/rl_training/generation' + str(generation) + '.chkpt')\n",
        "    for key in params:\n",
        "      if key in self._hyperparameter_keys:\n",
        "        setattr(self, '_' + key, params[key])\n",
        "      elif key == '_elapsed_generations':\n",
        "        self._elapsed_generations = params[key]\n",
        "      else:\n",
        "        self._population[int(key[len('individual_') - 1:])].load_state_dict(params[key])\n",
        "    params = None\n",
        "\n",
        "  def save(self):\n",
        "    params = self.get_population_dict()\n",
        "    for key in self._hyperparameter_keys:\n",
        "      params[key] = getattr(self, key)\n",
        "    params['_elapsed_generations'] = self._elapsed_generations\n",
        "    torch.save(params, 'drive/My Drive/rl_training/generation' + str(self._elapsed_generations) + '.chkpt')\n",
        "\n",
        "  def run_episode(self, env, obs, epsilon):\n",
        "    individual_to_run = self._tasks_remaining_in_generation.pop(0)\n",
        "    q = self._population[individual_to_run]\n",
        "    done = False\n",
        "    score = 0\n",
        "    while True:\n",
        "      a = q.sample_action(obs, -1)\n",
        "      new_obs, r, done, info = env.step(a)\n",
        "\n",
        "      score += r\n",
        "      obs = new_obs\n",
        "      if done:\n",
        "          break\n",
        "    q.add_score(score)\n",
        "    if self.no_tasks_left():\n",
        "      self.next_generation()\n",
        "      self.reset_remaining_tasks()\n",
        "    return score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYPyxqgfNHX"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "kSx3T55bcu6x",
        "outputId": "62990ff7-1369-4e04-d629-a4720b7c6d2a"
      },
      "source": [
        "\"\"\"\r\n",
        "BELOW IS THE GENETIC RUN CODE I WAS USING (that wasn't as effective as I hoped) along with some tests I wrote along the way\r\n",
        "to see if my genetic algorithms worked properly. By tweaking settings, it got a mean score of 198 of 1000 episodes and a max score of 850.\r\n",
        "\"\"\"\r\n",
        "\"\"\"\r\n",
        "# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\r\n",
        "env = gym.make('Gravitar-v0')\r\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\r\n",
        "\r\n",
        "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\r\n",
        "seed = 742\r\n",
        "torch.manual_seed(seed)\r\n",
        "env.seed(seed)\r\n",
        "random.seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "env.action_space.seed(seed)\r\n",
        "#g = GenomeQNet(env.observation_space.shape, env.action_space.n)\r\n",
        "#c = GeneticQNetContainer(env.observation_space.shape, env.action_space.n)\r\n",
        "#print(g.get_genome()[:100].shape)\r\n",
        "#print(c.get_genome_size())\r\n",
        "#params = c.build_params_from_genome(torch.zeros(c.get_genome_size()))\r\n",
        "#g.update_params(params)\r\n",
        "#print(g.get_genome()[:100].shape)\r\n",
        "#print(c.mutate_noise(torch.Tensor([0.5, 0.6, 0.7, 0.8, 0.9, 1.5,]), 0.5))\r\n",
        "#for i in range(100):\r\n",
        "#  print(c.breed(torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]), torch.Tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])))\r\n",
        "\r\n",
        "\r\n",
        "genetic_qnet = GeneticQNetContainer(env.observation_space.shape, env.action_space.n, {\r\n",
        "    'population_size': 50,\r\n",
        "    'cull_after': 0.6,\r\n",
        "    'mutation_chance': 0.9,\r\n",
        "    'use_crossover': False\r\n",
        "})\r\n",
        "genetic_qnet.init_random_population()\r\n",
        "score    = 0.0\r\n",
        "marking  = []\r\n",
        "#optimizer = optim.Adam(q.parameters(), lr=learning_rate)\r\n",
        "n_episodes = 101\r\n",
        "\r\n",
        "for n_episode in range(n_episodes):\r\n",
        "    epsilon = max(0.01, 0.08 - 0.01*(n_episode/200)) # linear annealing from 8% to 1%\r\n",
        "    s = env.reset()\r\n",
        "    score = genetic_qnet.run_episode(env, s, epsilon)\r\n",
        "    # do not change lines 44-48 here, they are for marking the submission log\r\n",
        "    marking.append(score)\r\n",
        "    if n_episode%100 == 0:\r\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\r\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\r\n",
        "        marking = []\r\n",
        "\r\n",
        "    # you can change this part, and print any data you like (so long as it doesn't start with \"marking\")\r\n",
        "    if n_episode%print_every==0 and n_episode!=0:\r\n",
        "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\\nenv = gym.make(\\'Gravitar-v0\\')\\nenv = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\\n\\n# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\\nseed = 742\\ntorch.manual_seed(seed)\\nenv.seed(seed)\\nrandom.seed(seed)\\nnp.random.seed(seed)\\nenv.action_space.seed(seed)\\n#g = GenomeQNet(env.observation_space.shape, env.action_space.n)\\n#c = GeneticQNetContainer(env.observation_space.shape, env.action_space.n)\\n#print(g.get_genome()[:100].shape)\\n#print(c.get_genome_size())\\n#params = c.build_params_from_genome(torch.zeros(c.get_genome_size()))\\n#g.update_params(params)\\n#print(g.get_genome()[:100].shape)\\n#print(c.mutate_noise(torch.Tensor([0.5, 0.6, 0.7, 0.8, 0.9, 1.5,]), 0.5))\\n#for i in range(100):\\n#  print(c.breed(torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]), torch.Tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])))\\n\\n\\ngenetic_qnet = GeneticQNetContainer(env.observation_space.shape, env.action_space.n, {\\n    \\'population_size\\': 50,\\n    \\'cull_after\\': 0.6,\\n    \\'mutation_chance\\': 0.9,\\n    \\'use_crossover\\': False\\n})\\ngenetic_qnet.init_random_population()\\nscore    = 0.0\\nmarking  = []\\n#optimizer = optim.Adam(q.parameters(), lr=learning_rate)\\nn_episodes = 101\\n\\nfor n_episode in range(n_episodes):\\n    epsilon = max(0.01, 0.08 - 0.01*(n_episode/200)) # linear annealing from 8% to 1%\\n    s = env.reset()\\n    score = genetic_qnet.run_episode(env, s, epsilon)\\n    # do not change lines 44-48 here, they are for marking the submission log\\n    marking.append(score)\\n    if n_episode%100 == 0:\\n        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\\n            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\\n        marking = []\\n\\n    # you can change this part, and print any data you like (so long as it doesn\\'t start with \"marking\")\\n    if n_episode%print_every==0 and n_episode!=0:\\n        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ck-chjFdScJ"
      },
      "source": [
        "**Train**\n",
        "\n",
        "← You can download the videos from the videos folder in the files on the left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5stHkFq4UztI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f375966c-2d48-4620-d4c3-eb30a4ff8430"
      },
      "source": [
        "# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\n",
        "env = gym.make('Gravitar-v0')\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "env = wrap_env(env)\n",
        "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\n",
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "AC_NET = ActorCritic(env, learning_rate, gamma)\n",
        "\n",
        "score    = 0.0\n",
        "marking  = []\n",
        "\n",
        "MAX_EPISODES = int(1e32)\n",
        "\n",
        "for n_episode in range(MAX_EPISODES):\n",
        "    score = 0.0\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    while not done:\n",
        "        for t in range(n_rollout):\n",
        "            prob = AC_NET.actor(torch.from_numpy(s).float().unsqueeze(0))\n",
        "            m = torch.distributions.Categorical(prob)\n",
        "            a = m.sample().item()\n",
        "            s_prime, r, done, info = env.step(a)\n",
        "            AC_NET.put_data((s,a,r,s_prime,done))\n",
        "            \n",
        "            s = s_prime\n",
        "            score += r\n",
        "            \n",
        "            if done:\n",
        "              break                     \n",
        "        \n",
        "        AC_NET.train_net()\n",
        "\n",
        "    # do not change lines 44-48 here, they are for marking the submission log\n",
        "    marking.append(score)\n",
        "    if n_episode%100 == 0:\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "        marking = []\n",
        "\n",
        "    # you can change this part, and print any data you like (so long as it doesn't start with \"marking\")\n",
        "    if n_episode%print_every==0 and n_episode!=0:\n",
        "        running_mean = -1\n",
        "        if len(marking) > 0:\n",
        "          running_mean = np.array(marking).mean()\n",
        "        print(\"episode: {}, score: {:.1f}, mean: {:.2f}\".format(n_episode, score, running_mean))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marking, episode: 0, score: 500.0, mean_score: 500.00, std_score: 0.00\n",
            "episode: 5, score: 0.0, mean: 210.00\n",
            "episode: 10, score: 250.0, mean: 180.00\n",
            "episode: 15, score: 0.0, mean: 183.33\n",
            "episode: 20, score: 250.0, mean: 160.00\n",
            "episode: 25, score: 250.0, mean: 176.00\n",
            "episode: 30, score: 0.0, mean: 185.00\n",
            "episode: 35, score: 100.0, mean: 171.43\n",
            "episode: 40, score: 200.0, mean: 167.50\n",
            "episode: 45, score: 0.0, mean: 151.11\n",
            "episode: 50, score: 500.0, mean: 153.00\n",
            "episode: 55, score: 0.0, mean: 159.09\n",
            "episode: 60, score: 0.0, mean: 157.50\n",
            "episode: 65, score: 250.0, mean: 163.85\n",
            "episode: 70, score: 100.0, mean: 174.29\n",
            "episode: 75, score: 100.0, mean: 172.00\n",
            "episode: 80, score: 500.0, mean: 175.00\n",
            "episode: 85, score: 250.0, mean: 172.94\n",
            "episode: 90, score: 300.0, mean: 172.78\n",
            "episode: 95, score: 0.0, mean: 172.11\n",
            "marking, episode: 100, score: 250.0, mean_score: 173.00, std_score: 185.53\n",
            "episode: 100, score: 250.0, mean: -1.00\n",
            "episode: 105, score: 350.0, mean: 240.00\n",
            "episode: 110, score: 0.0, mean: 165.00\n",
            "episode: 115, score: 500.0, mean: 193.33\n",
            "episode: 120, score: 250.0, mean: 252.50\n",
            "episode: 125, score: 0.0, mean: 222.00\n",
            "episode: 130, score: 500.0, mean: 225.00\n",
            "episode: 135, score: 0.0, mean: 210.00\n",
            "episode: 140, score: 250.0, mean: 192.50\n",
            "episode: 145, score: 0.0, mean: 184.44\n",
            "episode: 150, score: 0.0, mean: 173.00\n",
            "episode: 155, score: 450.0, mean: 186.36\n",
            "episode: 160, score: 700.0, mean: 195.83\n",
            "episode: 165, score: 250.0, mean: 200.00\n",
            "episode: 170, score: 0.0, mean: 205.00\n",
            "episode: 175, score: 350.0, mean: 199.33\n",
            "episode: 180, score: 250.0, mean: 197.50\n",
            "episode: 185, score: 250.0, mean: 200.00\n",
            "episode: 190, score: 0.0, mean: 192.22\n",
            "episode: 195, score: 0.0, mean: 197.37\n",
            "marking, episode: 200, score: 250.0, mean_score: 196.00, std_score: 204.53\n",
            "episode: 200, score: 250.0, mean: -1.00\n",
            "episode: 205, score: 0.0, mean: 360.00\n",
            "episode: 210, score: 100.0, mean: 315.00\n",
            "episode: 215, score: 250.0, mean: 276.67\n",
            "episode: 220, score: 500.0, mean: 297.50\n",
            "episode: 225, score: 0.0, mean: 266.00\n",
            "episode: 230, score: 100.0, mean: 243.33\n",
            "episode: 235, score: 250.0, mean: 237.14\n",
            "episode: 240, score: 0.0, mean: 222.50\n",
            "episode: 245, score: 0.0, mean: 211.11\n",
            "episode: 250, score: 350.0, mean: 211.00\n",
            "episode: 255, score: 100.0, mean: 212.73\n",
            "episode: 260, score: 250.0, mean: 221.67\n",
            "episode: 265, score: 200.0, mean: 222.31\n",
            "episode: 270, score: 250.0, mean: 225.71\n",
            "episode: 275, score: 550.0, mean: 228.00\n",
            "episode: 280, score: 250.0, mean: 236.25\n",
            "episode: 285, score: 250.0, mean: 235.29\n",
            "episode: 290, score: 350.0, mean: 234.44\n",
            "episode: 295, score: 250.0, mean: 228.42\n",
            "marking, episode: 300, score: 250.0, mean_score: 229.00, std_score: 208.11\n",
            "episode: 300, score: 250.0, mean: -1.00\n",
            "episode: 305, score: 0.0, mean: 50.00\n",
            "episode: 310, score: 100.0, mean: 160.00\n",
            "episode: 315, score: 350.0, mean: 183.33\n",
            "episode: 320, score: 500.0, mean: 220.00\n",
            "episode: 325, score: 0.0, mean: 200.00\n",
            "episode: 330, score: 250.0, mean: 195.00\n",
            "episode: 335, score: 0.0, mean: 194.29\n",
            "episode: 340, score: 500.0, mean: 188.75\n",
            "episode: 345, score: 500.0, mean: 181.11\n",
            "episode: 350, score: 250.0, mean: 185.00\n",
            "episode: 355, score: 0.0, mean: 181.82\n",
            "episode: 360, score: 200.0, mean: 181.67\n",
            "episode: 365, score: 500.0, mean: 179.23\n",
            "episode: 370, score: 250.0, mean: 185.00\n",
            "episode: 375, score: 0.0, mean: 192.00\n",
            "episode: 380, score: 250.0, mean: 201.88\n",
            "episode: 385, score: 0.0, mean: 198.82\n",
            "episode: 390, score: 250.0, mean: 200.00\n",
            "episode: 395, score: 0.0, mean: 194.74\n",
            "marking, episode: 400, score: 500.0, mean_score: 203.50, std_score: 188.85\n",
            "episode: 400, score: 500.0, mean: -1.00\n",
            "episode: 405, score: 100.0, mean: 190.00\n",
            "episode: 410, score: 100.0, mean: 215.00\n",
            "episode: 415, score: 250.0, mean: 200.00\n",
            "episode: 420, score: 500.0, mean: 220.00\n",
            "episode: 425, score: 0.0, mean: 234.00\n",
            "episode: 430, score: 0.0, mean: 223.33\n",
            "episode: 435, score: 200.0, mean: 210.00\n",
            "episode: 440, score: 300.0, mean: 206.25\n",
            "episode: 445, score: 0.0, mean: 196.67\n",
            "episode: 450, score: 250.0, mean: 218.00\n",
            "episode: 455, score: 500.0, mean: 227.27\n",
            "episode: 460, score: 450.0, mean: 232.50\n",
            "episode: 465, score: 100.0, mean: 219.23\n",
            "episode: 470, score: 0.0, mean: 227.86\n",
            "episode: 475, score: 100.0, mean: 222.00\n",
            "episode: 480, score: 100.0, mean: 215.62\n",
            "episode: 485, score: 750.0, mean: 218.82\n",
            "episode: 490, score: 250.0, mean: 225.56\n",
            "episode: 495, score: 200.0, mean: 218.42\n",
            "marking, episode: 500, score: 350.0, mean_score: 217.00, std_score: 197.01\n",
            "episode: 500, score: 350.0, mean: -1.00\n",
            "episode: 505, score: 0.0, mean: 310.00\n",
            "episode: 510, score: 100.0, mean: 250.00\n",
            "episode: 515, score: 0.0, mean: 216.67\n",
            "episode: 520, score: 0.0, mean: 172.50\n",
            "episode: 525, score: 0.0, mean: 186.00\n",
            "episode: 530, score: 200.0, mean: 208.33\n",
            "episode: 535, score: 100.0, mean: 191.43\n",
            "episode: 540, score: 100.0, mean: 186.25\n",
            "episode: 545, score: 500.0, mean: 197.78\n",
            "episode: 550, score: 0.0, mean: 205.00\n",
            "episode: 555, score: 0.0, mean: 194.55\n",
            "episode: 560, score: 350.0, mean: 190.00\n",
            "episode: 565, score: 600.0, mean: 196.15\n",
            "episode: 570, score: 0.0, mean: 194.29\n",
            "episode: 575, score: 0.0, mean: 194.00\n",
            "episode: 580, score: 250.0, mean: 191.25\n",
            "episode: 585, score: 0.0, mean: 198.82\n",
            "episode: 590, score: 0.0, mean: 197.22\n",
            "episode: 595, score: 250.0, mean: 193.16\n",
            "marking, episode: 600, score: 250.0, mean_score: 194.50, std_score: 201.23\n",
            "episode: 600, score: 250.0, mean: -1.00\n",
            "episode: 605, score: 100.0, mean: 240.00\n",
            "episode: 610, score: 500.0, mean: 205.00\n",
            "episode: 615, score: 350.0, mean: 323.33\n",
            "episode: 620, score: 350.0, mean: 322.50\n",
            "episode: 625, score: 100.0, mean: 302.00\n",
            "episode: 630, score: 750.0, mean: 301.67\n",
            "episode: 635, score: 500.0, mean: 287.14\n",
            "episode: 640, score: 0.0, mean: 268.75\n",
            "episode: 645, score: 0.0, mean: 266.67\n",
            "episode: 650, score: 250.0, mean: 260.00\n",
            "episode: 655, score: 100.0, mean: 247.27\n",
            "episode: 660, score: 250.0, mean: 242.50\n",
            "episode: 665, score: 250.0, mean: 227.69\n",
            "episode: 670, score: 250.0, mean: 223.57\n",
            "episode: 675, score: 250.0, mean: 223.33\n",
            "episode: 680, score: 350.0, mean: 216.88\n",
            "episode: 685, score: 0.0, mean: 215.88\n",
            "episode: 690, score: 250.0, mean: 212.22\n",
            "episode: 695, score: 0.0, mean: 203.68\n",
            "marking, episode: 700, score: 250.0, mean_score: 204.50, std_score: 265.72\n",
            "episode: 700, score: 250.0, mean: -1.00\n",
            "episode: 705, score: 0.0, mean: 170.00\n",
            "episode: 710, score: 0.0, mean: 155.00\n",
            "episode: 715, score: 500.0, mean: 236.67\n",
            "episode: 720, score: 250.0, mean: 227.50\n",
            "episode: 725, score: 500.0, mean: 238.00\n",
            "episode: 730, score: 600.0, mean: 250.00\n",
            "episode: 735, score: 250.0, mean: 231.43\n",
            "episode: 740, score: 250.0, mean: 236.25\n",
            "episode: 745, score: 350.0, mean: 233.33\n",
            "episode: 750, score: 100.0, mean: 222.00\n",
            "episode: 755, score: 100.0, mean: 217.27\n",
            "episode: 760, score: 0.0, mean: 215.83\n",
            "episode: 765, score: 0.0, mean: 219.23\n",
            "episode: 770, score: 600.0, mean: 215.71\n",
            "episode: 775, score: 0.0, mean: 212.00\n",
            "episode: 780, score: 0.0, mean: 213.75\n",
            "episode: 785, score: 750.0, mean: 218.24\n",
            "episode: 790, score: 500.0, mean: 221.11\n",
            "episode: 795, score: 0.0, mean: 217.89\n",
            "marking, episode: 800, score: 0.0, mean_score: 220.50, std_score: 214.72\n",
            "episode: 800, score: 0.0, mean: -1.00\n",
            "episode: 805, score: 500.0, mean: 200.00\n",
            "episode: 810, score: 600.0, mean: 235.00\n",
            "episode: 815, score: 250.0, mean: 230.00\n",
            "episode: 820, score: 350.0, mean: 232.50\n",
            "episode: 825, score: 0.0, mean: 220.00\n",
            "episode: 830, score: 0.0, mean: 191.67\n",
            "episode: 835, score: 250.0, mean: 195.71\n",
            "episode: 840, score: 250.0, mean: 192.50\n",
            "episode: 845, score: 0.0, mean: 201.11\n",
            "episode: 850, score: 250.0, mean: 200.00\n",
            "episode: 855, score: 100.0, mean: 188.18\n",
            "episode: 860, score: 350.0, mean: 190.83\n",
            "episode: 865, score: 250.0, mean: 202.31\n",
            "episode: 870, score: 350.0, mean: 217.86\n",
            "episode: 875, score: 250.0, mean: 218.00\n",
            "episode: 880, score: 0.0, mean: 215.00\n",
            "episode: 885, score: 0.0, mean: 212.35\n",
            "episode: 890, score: 0.0, mean: 209.44\n",
            "episode: 895, score: 200.0, mean: 210.53\n",
            "marking, episode: 900, score: 500.0, mean_score: 215.00, std_score: 215.35\n",
            "episode: 900, score: 500.0, mean: -1.00\n",
            "episode: 905, score: 0.0, mean: 70.00\n",
            "episode: 910, score: 500.0, mean: 220.00\n",
            "episode: 915, score: 0.0, mean: 203.33\n",
            "episode: 920, score: 250.0, mean: 242.50\n",
            "episode: 925, score: 500.0, mean: 258.00\n",
            "episode: 930, score: 250.0, mean: 260.00\n",
            "episode: 935, score: 250.0, mean: 258.57\n",
            "episode: 940, score: 250.0, mean: 235.00\n",
            "episode: 945, score: 0.0, mean: 227.78\n",
            "episode: 950, score: 0.0, mean: 207.00\n",
            "episode: 955, score: 250.0, mean: 207.27\n",
            "episode: 960, score: 250.0, mean: 198.33\n",
            "episode: 965, score: 0.0, mean: 190.77\n",
            "episode: 970, score: 0.0, mean: 178.57\n",
            "episode: 975, score: 250.0, mean: 186.67\n",
            "episode: 980, score: 0.0, mean: 184.38\n",
            "episode: 985, score: 0.0, mean: 177.65\n",
            "episode: 990, score: 0.0, mean: 171.67\n",
            "episode: 995, score: 0.0, mean: 177.89\n",
            "marking, episode: 1000, score: 450.0, mean_score: 183.00, std_score: 220.71\n",
            "episode: 1000, score: 450.0, mean: -1.00\n",
            "episode: 1005, score: 500.0, mean: 370.00\n",
            "episode: 1010, score: 250.0, mean: 295.00\n",
            "episode: 1015, score: 500.0, mean: 300.00\n",
            "episode: 1020, score: 250.0, mean: 267.50\n",
            "episode: 1025, score: 100.0, mean: 248.00\n",
            "episode: 1030, score: 0.0, mean: 243.33\n",
            "episode: 1035, score: 0.0, mean: 230.00\n",
            "episode: 1040, score: 0.0, mean: 210.00\n",
            "episode: 1045, score: 0.0, mean: 197.78\n",
            "episode: 1050, score: 0.0, mean: 200.00\n",
            "episode: 1055, score: 500.0, mean: 192.73\n",
            "episode: 1060, score: 100.0, mean: 190.83\n",
            "episode: 1065, score: 250.0, mean: 189.23\n",
            "episode: 1070, score: 200.0, mean: 192.86\n",
            "episode: 1075, score: 250.0, mean: 190.00\n",
            "episode: 1080, score: 100.0, mean: 186.88\n",
            "episode: 1085, score: 500.0, mean: 198.82\n",
            "episode: 1090, score: 0.0, mean: 190.56\n",
            "episode: 1095, score: 250.0, mean: 191.05\n",
            "marking, episode: 1100, score: 250.0, mean_score: 187.50, std_score: 205.72\n",
            "episode: 1100, score: 250.0, mean: -1.00\n",
            "episode: 1105, score: 350.0, mean: 90.00\n",
            "episode: 1110, score: 250.0, mean: 170.00\n",
            "episode: 1115, score: 250.0, mean: 210.00\n",
            "episode: 1120, score: 250.0, mean: 220.00\n",
            "episode: 1125, score: 0.0, mean: 196.00\n",
            "episode: 1130, score: 250.0, mean: 186.67\n",
            "episode: 1135, score: 250.0, mean: 180.00\n",
            "episode: 1140, score: 250.0, mean: 187.50\n",
            "episode: 1145, score: 250.0, mean: 194.44\n",
            "episode: 1150, score: 250.0, mean: 190.00\n",
            "episode: 1155, score: 250.0, mean: 188.18\n",
            "episode: 1160, score: 500.0, mean: 203.33\n",
            "episode: 1165, score: 0.0, mean: 200.00\n",
            "episode: 1170, score: 0.0, mean: 196.43\n",
            "episode: 1175, score: 250.0, mean: 196.67\n",
            "episode: 1180, score: 250.0, mean: 191.88\n",
            "episode: 1185, score: 250.0, mean: 191.76\n",
            "episode: 1190, score: 0.0, mean: 184.44\n",
            "episode: 1195, score: 250.0, mean: 187.89\n",
            "marking, episode: 1200, score: 0.0, mean_score: 190.50, std_score: 179.68\n",
            "episode: 1200, score: 0.0, mean: -1.00\n",
            "episode: 1205, score: 500.0, mean: 120.00\n",
            "episode: 1210, score: 250.0, mean: 230.00\n",
            "episode: 1215, score: 0.0, mean: 216.67\n",
            "episode: 1220, score: 0.0, mean: 190.00\n",
            "episode: 1225, score: 100.0, mean: 190.00\n",
            "episode: 1230, score: 250.0, mean: 200.00\n",
            "episode: 1235, score: 0.0, mean: 180.00\n",
            "episode: 1240, score: 500.0, mean: 191.25\n",
            "episode: 1245, score: 0.0, mean: 183.33\n",
            "episode: 1250, score: 250.0, mean: 187.00\n",
            "episode: 1255, score: 0.0, mean: 185.45\n",
            "episode: 1260, score: 0.0, mean: 173.33\n",
            "episode: 1265, score: 0.0, mean: 176.92\n",
            "episode: 1270, score: 0.0, mean: 182.86\n",
            "episode: 1275, score: 500.0, mean: 186.67\n",
            "episode: 1280, score: 250.0, mean: 215.62\n",
            "episode: 1285, score: 0.0, mean: 210.00\n",
            "episode: 1290, score: 250.0, mean: 208.89\n",
            "episode: 1295, score: 0.0, mean: 203.16\n",
            "marking, episode: 1300, score: 250.0, mean_score: 208.50, std_score: 270.74\n",
            "episode: 1300, score: 250.0, mean: -1.00\n",
            "episode: 1305, score: 0.0, mean: 100.00\n",
            "episode: 1310, score: 250.0, mean: 135.00\n",
            "episode: 1315, score: 0.0, mean: 113.33\n",
            "episode: 1320, score: 250.0, mean: 107.50\n",
            "episode: 1325, score: 500.0, mean: 150.00\n",
            "episode: 1330, score: 250.0, mean: 145.00\n",
            "episode: 1335, score: 0.0, mean: 141.43\n",
            "episode: 1340, score: 0.0, mean: 145.00\n",
            "episode: 1345, score: 350.0, mean: 147.78\n",
            "episode: 1350, score: 100.0, mean: 152.00\n",
            "episode: 1355, score: 0.0, mean: 152.73\n",
            "episode: 1360, score: 0.0, mean: 140.00\n",
            "episode: 1365, score: 250.0, mean: 144.62\n",
            "episode: 1370, score: 0.0, mean: 144.29\n",
            "episode: 1375, score: 0.0, mean: 139.33\n",
            "episode: 1380, score: 250.0, mean: 138.12\n",
            "episode: 1385, score: 0.0, mean: 135.88\n",
            "episode: 1390, score: 100.0, mean: 140.56\n",
            "episode: 1395, score: 100.0, mean: 138.95\n",
            "marking, episode: 1400, score: 250.0, mean_score: 149.50, std_score: 179.23\n",
            "episode: 1400, score: 250.0, mean: -1.00\n",
            "episode: 1405, score: 250.0, mean: 200.00\n",
            "episode: 1410, score: 0.0, mean: 170.00\n",
            "episode: 1415, score: 250.0, mean: 163.33\n",
            "episode: 1420, score: 250.0, mean: 190.00\n",
            "episode: 1425, score: 0.0, mean: 176.00\n",
            "episode: 1430, score: 0.0, mean: 170.00\n",
            "episode: 1435, score: 750.0, mean: 205.71\n",
            "episode: 1440, score: 450.0, mean: 208.75\n",
            "episode: 1445, score: 500.0, mean: 207.78\n",
            "episode: 1450, score: 250.0, mean: 213.00\n",
            "episode: 1455, score: 250.0, mean: 207.27\n",
            "episode: 1460, score: 250.0, mean: 206.67\n",
            "episode: 1465, score: 600.0, mean: 214.62\n",
            "episode: 1470, score: 100.0, mean: 223.57\n",
            "episode: 1475, score: 200.0, mean: 222.67\n",
            "episode: 1480, score: 0.0, mean: 217.50\n",
            "episode: 1485, score: 100.0, mean: 210.00\n",
            "episode: 1490, score: 200.0, mean: 207.22\n",
            "episode: 1495, score: 100.0, mean: 203.68\n",
            "marking, episode: 1500, score: 250.0, mean_score: 199.00, std_score: 169.41\n",
            "episode: 1500, score: 250.0, mean: -1.00\n",
            "episode: 1505, score: 0.0, mean: 250.00\n",
            "episode: 1510, score: 250.0, mean: 195.00\n",
            "episode: 1515, score: 0.0, mean: 153.33\n",
            "episode: 1520, score: 0.0, mean: 120.00\n",
            "episode: 1525, score: 100.0, mean: 150.00\n",
            "episode: 1530, score: 0.0, mean: 160.00\n",
            "episode: 1535, score: 100.0, mean: 167.14\n",
            "episode: 1540, score: 0.0, mean: 161.25\n",
            "episode: 1545, score: 100.0, mean: 162.22\n",
            "episode: 1550, score: 0.0, mean: 156.00\n",
            "episode: 1555, score: 250.0, mean: 164.55\n",
            "episode: 1560, score: 500.0, mean: 169.17\n",
            "episode: 1565, score: 700.0, mean: 174.62\n",
            "episode: 1570, score: 0.0, mean: 169.29\n",
            "episode: 1575, score: 250.0, mean: 171.33\n",
            "episode: 1580, score: 500.0, mean: 181.88\n",
            "episode: 1585, score: 0.0, mean: 182.94\n",
            "episode: 1590, score: 0.0, mean: 175.56\n",
            "episode: 1595, score: 0.0, mean: 169.47\n",
            "marking, episode: 1600, score: 0.0, mean_score: 171.00, std_score: 190.94\n",
            "episode: 1600, score: 0.0, mean: -1.00\n",
            "episode: 1605, score: 0.0, mean: 100.00\n",
            "episode: 1610, score: 0.0, mean: 75.00\n",
            "episode: 1615, score: 0.0, mean: 66.67\n",
            "episode: 1620, score: 0.0, mean: 112.50\n",
            "episode: 1625, score: 0.0, mean: 110.00\n",
            "episode: 1630, score: 0.0, mean: 100.00\n",
            "episode: 1635, score: 500.0, mean: 128.57\n",
            "episode: 1640, score: 0.0, mean: 125.00\n",
            "episode: 1645, score: 0.0, mean: 122.22\n",
            "episode: 1650, score: 0.0, mean: 120.00\n",
            "episode: 1655, score: 500.0, mean: 127.27\n",
            "episode: 1660, score: 100.0, mean: 135.00\n",
            "episode: 1665, score: 250.0, mean: 136.15\n",
            "episode: 1670, score: 0.0, mean: 135.00\n",
            "episode: 1675, score: 0.0, mean: 129.33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d7a4646b9869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mAC_NET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# do not change lines 44-48 here, they are for marking the submission log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8c68b4f30f3e>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m       \u001b[0mtd_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8c68b4f30f3e>\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0mdone_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                               \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prime_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                               \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfLNEPS4yZ5_"
      },
      "source": [
        "Information about the Genetic Algorithm (GA)\r\n",
        "\r\n",
        "I think GAs are beautiful given that they learn based on the proven methods in nature and I thought they might work well for Gravitar considering the reward sparsity. I found this paper https://arxiv.org/pdf/1712.06567.pdf which showed the use of GAs for solving Atari games (and proved that GAs are not just random search in this space) and it looked promising so I wrote this container for running genome operations on neural nets. However, their population size was 1K and mine could only be 50 because I didn't use their compression technique and was running out of RAM (and didn't have time to run circa 1000 episodes for 1 generation and risk little improvement over hours). Their implementation just added noise to each weight with a factor. I tried improving this by adding ordered crossover of flattened neural nets but this didn't seem to work well which may be because it broke correlation between parts of the net so ended up just using their noise method with a high mutation chance. Interestingly, the use of LSTMCells seemed to make the agent perform better but this may be coincidental. If they actually do help it could be because GA is gradient-free (hence its fast run time per episode) and the LSTM somehow acts as inter-generational memory. The paper did use the preprocessing methods that I ended up using for the A2C but they didn't work properly with my implementation so the GA was slower than it should have been. However, it did hit a competitive mean score with my A2C and peaked at a score of 850-900 but unfortunately started to fall again which suggests the mutation rate needed to be adjusted, especially considering I added an aggressive culling scheme and mutated only from the most elite population members to attempt to improve convergence. I couldn't be sure that my implementation would actually converge hence I abandoned it in favour of a more traditional method (Duelling DQN first then A2C)."
      ]
    }
  ]
}